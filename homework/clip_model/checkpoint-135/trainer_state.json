{
  "best_global_step": null,
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 0.10022271714922049,
  "eval_steps": 500,
  "global_step": 135,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.0007423904974016332,
      "grad_norm": 6.312788486480713,
      "learning_rate": 0.0001,
      "loss": 4.4486,
      "step": 1
    },
    {
      "epoch": 0.0014847809948032665,
      "grad_norm": 5.491036891937256,
      "learning_rate": 9.925925925925926e-05,
      "loss": 4.3302,
      "step": 2
    },
    {
      "epoch": 0.0022271714922048997,
      "grad_norm": 7.396425724029541,
      "learning_rate": 9.851851851851852e-05,
      "loss": 4.3981,
      "step": 3
    },
    {
      "epoch": 0.002969561989606533,
      "grad_norm": 4.407693862915039,
      "learning_rate": 9.777777777777778e-05,
      "loss": 4.2804,
      "step": 4
    },
    {
      "epoch": 0.003711952487008166,
      "grad_norm": 4.981956958770752,
      "learning_rate": 9.703703703703704e-05,
      "loss": 4.2568,
      "step": 5
    },
    {
      "epoch": 0.004454342984409799,
      "grad_norm": 4.145549297332764,
      "learning_rate": 9.62962962962963e-05,
      "loss": 4.2278,
      "step": 6
    },
    {
      "epoch": 0.005196733481811433,
      "grad_norm": 3.9977405071258545,
      "learning_rate": 9.555555555555557e-05,
      "loss": 4.2464,
      "step": 7
    },
    {
      "epoch": 0.005939123979213066,
      "grad_norm": 3.6345434188842773,
      "learning_rate": 9.481481481481483e-05,
      "loss": 4.2402,
      "step": 8
    },
    {
      "epoch": 0.0066815144766146995,
      "grad_norm": 5.827762603759766,
      "learning_rate": 9.407407407407408e-05,
      "loss": 4.2861,
      "step": 9
    },
    {
      "epoch": 0.007423904974016332,
      "grad_norm": 3.4757583141326904,
      "learning_rate": 9.333333333333334e-05,
      "loss": 4.2589,
      "step": 10
    },
    {
      "epoch": 0.008166295471417966,
      "grad_norm": 2.709885358810425,
      "learning_rate": 9.25925925925926e-05,
      "loss": 4.2146,
      "step": 11
    },
    {
      "epoch": 0.008908685968819599,
      "grad_norm": 2.576899528503418,
      "learning_rate": 9.185185185185186e-05,
      "loss": 4.2372,
      "step": 12
    },
    {
      "epoch": 0.009651076466221232,
      "grad_norm": 2.4843099117279053,
      "learning_rate": 9.111111111111112e-05,
      "loss": 4.2224,
      "step": 13
    },
    {
      "epoch": 0.010393466963622866,
      "grad_norm": 1.9719487428665161,
      "learning_rate": 9.037037037037038e-05,
      "loss": 4.1876,
      "step": 14
    },
    {
      "epoch": 0.011135857461024499,
      "grad_norm": 1.8684693574905396,
      "learning_rate": 8.962962962962963e-05,
      "loss": 4.1854,
      "step": 15
    },
    {
      "epoch": 0.011878247958426132,
      "grad_norm": 1.5085066556930542,
      "learning_rate": 8.888888888888889e-05,
      "loss": 4.1867,
      "step": 16
    },
    {
      "epoch": 0.012620638455827766,
      "grad_norm": 1.283274531364441,
      "learning_rate": 8.814814814814815e-05,
      "loss": 4.1631,
      "step": 17
    },
    {
      "epoch": 0.013363028953229399,
      "grad_norm": 1.6146869659423828,
      "learning_rate": 8.740740740740741e-05,
      "loss": 4.1788,
      "step": 18
    },
    {
      "epoch": 0.014105419450631032,
      "grad_norm": 1.6581099033355713,
      "learning_rate": 8.666666666666667e-05,
      "loss": 4.1634,
      "step": 19
    },
    {
      "epoch": 0.014847809948032665,
      "grad_norm": 1.2638708353042603,
      "learning_rate": 8.592592592592593e-05,
      "loss": 4.1721,
      "step": 20
    },
    {
      "epoch": 0.015590200445434299,
      "grad_norm": 1.2543892860412598,
      "learning_rate": 8.518518518518518e-05,
      "loss": 4.1641,
      "step": 21
    },
    {
      "epoch": 0.016332590942835932,
      "grad_norm": 1.0709145069122314,
      "learning_rate": 8.444444444444444e-05,
      "loss": 4.1674,
      "step": 22
    },
    {
      "epoch": 0.017074981440237565,
      "grad_norm": 1.101203441619873,
      "learning_rate": 8.37037037037037e-05,
      "loss": 4.1682,
      "step": 23
    },
    {
      "epoch": 0.017817371937639197,
      "grad_norm": 0.9942585825920105,
      "learning_rate": 8.296296296296296e-05,
      "loss": 4.1633,
      "step": 24
    },
    {
      "epoch": 0.01855976243504083,
      "grad_norm": 2.633100748062134,
      "learning_rate": 8.222222222222222e-05,
      "loss": 4.1685,
      "step": 25
    },
    {
      "epoch": 0.019302152932442463,
      "grad_norm": 0.6926470398902893,
      "learning_rate": 8.148148148148148e-05,
      "loss": 4.1507,
      "step": 26
    },
    {
      "epoch": 0.0200445434298441,
      "grad_norm": 1.3547115325927734,
      "learning_rate": 8.074074074074075e-05,
      "loss": 4.1569,
      "step": 27
    },
    {
      "epoch": 0.020786933927245732,
      "grad_norm": 0.8474087715148926,
      "learning_rate": 8e-05,
      "loss": 4.1522,
      "step": 28
    },
    {
      "epoch": 0.021529324424647365,
      "grad_norm": 0.7614284753799438,
      "learning_rate": 7.925925925925926e-05,
      "loss": 4.1479,
      "step": 29
    },
    {
      "epoch": 0.022271714922048998,
      "grad_norm": 0.8158056139945984,
      "learning_rate": 7.851851851851852e-05,
      "loss": 4.1452,
      "step": 30
    },
    {
      "epoch": 0.02301410541945063,
      "grad_norm": 0.6341930031776428,
      "learning_rate": 7.777777777777778e-05,
      "loss": 4.1462,
      "step": 31
    },
    {
      "epoch": 0.023756495916852263,
      "grad_norm": 0.7527981400489807,
      "learning_rate": 7.703703703703704e-05,
      "loss": 4.141,
      "step": 32
    },
    {
      "epoch": 0.024498886414253896,
      "grad_norm": 0.9216168522834778,
      "learning_rate": 7.62962962962963e-05,
      "loss": 4.1547,
      "step": 33
    },
    {
      "epoch": 0.025241276911655532,
      "grad_norm": 0.7973789572715759,
      "learning_rate": 7.555555555555556e-05,
      "loss": 4.1418,
      "step": 34
    },
    {
      "epoch": 0.025983667409057165,
      "grad_norm": 0.8094921708106995,
      "learning_rate": 7.481481481481481e-05,
      "loss": 4.1475,
      "step": 35
    },
    {
      "epoch": 0.026726057906458798,
      "grad_norm": 0.8758531212806702,
      "learning_rate": 7.407407407407407e-05,
      "loss": 4.15,
      "step": 36
    },
    {
      "epoch": 0.02746844840386043,
      "grad_norm": 0.9401751756668091,
      "learning_rate": 7.333333333333333e-05,
      "loss": 4.1542,
      "step": 37
    },
    {
      "epoch": 0.028210838901262063,
      "grad_norm": 1.362608551979065,
      "learning_rate": 7.25925925925926e-05,
      "loss": 4.14,
      "step": 38
    },
    {
      "epoch": 0.028953229398663696,
      "grad_norm": 1.0620290040969849,
      "learning_rate": 7.185185185185186e-05,
      "loss": 4.1462,
      "step": 39
    },
    {
      "epoch": 0.02969561989606533,
      "grad_norm": 1.4495335817337036,
      "learning_rate": 7.111111111111112e-05,
      "loss": 4.1606,
      "step": 40
    },
    {
      "epoch": 0.030438010393466965,
      "grad_norm": 1.519376277923584,
      "learning_rate": 7.037037037037038e-05,
      "loss": 4.1348,
      "step": 41
    },
    {
      "epoch": 0.031180400890868598,
      "grad_norm": 1.8956420421600342,
      "learning_rate": 6.962962962962964e-05,
      "loss": 4.1198,
      "step": 42
    },
    {
      "epoch": 0.03192279138827023,
      "grad_norm": 1.1055164337158203,
      "learning_rate": 6.88888888888889e-05,
      "loss": 4.1364,
      "step": 43
    },
    {
      "epoch": 0.032665181885671864,
      "grad_norm": 1.7183127403259277,
      "learning_rate": 6.814814814814815e-05,
      "loss": 4.1258,
      "step": 44
    },
    {
      "epoch": 0.0334075723830735,
      "grad_norm": 1.5458132028579712,
      "learning_rate": 6.740740740740741e-05,
      "loss": 4.1709,
      "step": 45
    },
    {
      "epoch": 0.03414996288047513,
      "grad_norm": 1.6599129438400269,
      "learning_rate": 6.666666666666667e-05,
      "loss": 4.0858,
      "step": 46
    },
    {
      "epoch": 0.034892353377876766,
      "grad_norm": 1.5012167692184448,
      "learning_rate": 6.592592592592593e-05,
      "loss": 4.1376,
      "step": 47
    },
    {
      "epoch": 0.035634743875278395,
      "grad_norm": 1.82027006149292,
      "learning_rate": 6.51851851851852e-05,
      "loss": 4.1397,
      "step": 48
    },
    {
      "epoch": 0.03637713437268003,
      "grad_norm": 1.6871405839920044,
      "learning_rate": 6.444444444444446e-05,
      "loss": 4.1269,
      "step": 49
    },
    {
      "epoch": 0.03711952487008166,
      "grad_norm": 2.0218472480773926,
      "learning_rate": 6.37037037037037e-05,
      "loss": 4.1551,
      "step": 50
    },
    {
      "epoch": 0.0378619153674833,
      "grad_norm": 1.8337037563323975,
      "learning_rate": 6.296296296296296e-05,
      "loss": 4.1045,
      "step": 51
    },
    {
      "epoch": 0.038604305864884926,
      "grad_norm": 1.7808488607406616,
      "learning_rate": 6.222222222222222e-05,
      "loss": 4.1189,
      "step": 52
    },
    {
      "epoch": 0.03934669636228656,
      "grad_norm": 2.021185874938965,
      "learning_rate": 6.148148148148148e-05,
      "loss": 4.1216,
      "step": 53
    },
    {
      "epoch": 0.0400890868596882,
      "grad_norm": 2.0861833095550537,
      "learning_rate": 6.074074074074074e-05,
      "loss": 4.1183,
      "step": 54
    },
    {
      "epoch": 0.04083147735708983,
      "grad_norm": 2.132490873336792,
      "learning_rate": 6e-05,
      "loss": 4.1293,
      "step": 55
    },
    {
      "epoch": 0.041573867854491464,
      "grad_norm": 2.3746049404144287,
      "learning_rate": 5.925925925925926e-05,
      "loss": 4.109,
      "step": 56
    },
    {
      "epoch": 0.042316258351893093,
      "grad_norm": 2.8414907455444336,
      "learning_rate": 5.851851851851852e-05,
      "loss": 4.0941,
      "step": 57
    },
    {
      "epoch": 0.04305864884929473,
      "grad_norm": 2.257232427597046,
      "learning_rate": 5.7777777777777776e-05,
      "loss": 4.1104,
      "step": 58
    },
    {
      "epoch": 0.04380103934669636,
      "grad_norm": 2.1280717849731445,
      "learning_rate": 5.703703703703704e-05,
      "loss": 4.109,
      "step": 59
    },
    {
      "epoch": 0.044543429844097995,
      "grad_norm": 2.7717764377593994,
      "learning_rate": 5.62962962962963e-05,
      "loss": 4.1087,
      "step": 60
    },
    {
      "epoch": 0.04528582034149963,
      "grad_norm": 2.5712194442749023,
      "learning_rate": 5.555555555555556e-05,
      "loss": 4.0807,
      "step": 61
    },
    {
      "epoch": 0.04602821083890126,
      "grad_norm": 2.7704648971557617,
      "learning_rate": 5.4814814814814817e-05,
      "loss": 4.065,
      "step": 62
    },
    {
      "epoch": 0.0467706013363029,
      "grad_norm": 2.792036771774292,
      "learning_rate": 5.4074074074074075e-05,
      "loss": 4.0805,
      "step": 63
    },
    {
      "epoch": 0.047512991833704527,
      "grad_norm": 2.557739496231079,
      "learning_rate": 5.333333333333333e-05,
      "loss": 4.04,
      "step": 64
    },
    {
      "epoch": 0.04825538233110616,
      "grad_norm": 2.442322254180908,
      "learning_rate": 5.259259259259259e-05,
      "loss": 4.0814,
      "step": 65
    },
    {
      "epoch": 0.04899777282850779,
      "grad_norm": 2.688445806503296,
      "learning_rate": 5.185185185185185e-05,
      "loss": 4.0893,
      "step": 66
    },
    {
      "epoch": 0.04974016332590943,
      "grad_norm": 2.6323344707489014,
      "learning_rate": 5.111111111111111e-05,
      "loss": 4.0268,
      "step": 67
    },
    {
      "epoch": 0.050482553823311065,
      "grad_norm": 3.9666879177093506,
      "learning_rate": 5.0370370370370366e-05,
      "loss": 4.0943,
      "step": 68
    },
    {
      "epoch": 0.051224944320712694,
      "grad_norm": 3.4660258293151855,
      "learning_rate": 4.962962962962963e-05,
      "loss": 4.0608,
      "step": 69
    },
    {
      "epoch": 0.05196733481811433,
      "grad_norm": 2.815753221511841,
      "learning_rate": 4.888888888888889e-05,
      "loss": 4.0776,
      "step": 70
    },
    {
      "epoch": 0.05270972531551596,
      "grad_norm": 3.1849098205566406,
      "learning_rate": 4.814814814814815e-05,
      "loss": 4.0979,
      "step": 71
    },
    {
      "epoch": 0.053452115812917596,
      "grad_norm": 5.373621940612793,
      "learning_rate": 4.740740740740741e-05,
      "loss": 3.9647,
      "step": 72
    },
    {
      "epoch": 0.054194506310319225,
      "grad_norm": 3.0161170959472656,
      "learning_rate": 4.666666666666667e-05,
      "loss": 4.0438,
      "step": 73
    },
    {
      "epoch": 0.05493689680772086,
      "grad_norm": 3.429452419281006,
      "learning_rate": 4.592592592592593e-05,
      "loss": 4.02,
      "step": 74
    },
    {
      "epoch": 0.0556792873051225,
      "grad_norm": 3.2314250469207764,
      "learning_rate": 4.518518518518519e-05,
      "loss": 4.0397,
      "step": 75
    },
    {
      "epoch": 0.05642167780252413,
      "grad_norm": 3.296560049057007,
      "learning_rate": 4.4444444444444447e-05,
      "loss": 4.0556,
      "step": 76
    },
    {
      "epoch": 0.05716406829992576,
      "grad_norm": 3.6389880180358887,
      "learning_rate": 4.3703703703703705e-05,
      "loss": 4.0491,
      "step": 77
    },
    {
      "epoch": 0.05790645879732739,
      "grad_norm": 4.155650615692139,
      "learning_rate": 4.296296296296296e-05,
      "loss": 4.0751,
      "step": 78
    },
    {
      "epoch": 0.05864884929472903,
      "grad_norm": 5.189559459686279,
      "learning_rate": 4.222222222222222e-05,
      "loss": 4.0935,
      "step": 79
    },
    {
      "epoch": 0.05939123979213066,
      "grad_norm": 5.597157955169678,
      "learning_rate": 4.148148148148148e-05,
      "loss": 4.0812,
      "step": 80
    },
    {
      "epoch": 0.060133630289532294,
      "grad_norm": 3.622117042541504,
      "learning_rate": 4.074074074074074e-05,
      "loss": 4.0261,
      "step": 81
    },
    {
      "epoch": 0.06087602078693393,
      "grad_norm": 4.771020889282227,
      "learning_rate": 4e-05,
      "loss": 4.0285,
      "step": 82
    },
    {
      "epoch": 0.06161841128433556,
      "grad_norm": 4.180675029754639,
      "learning_rate": 3.925925925925926e-05,
      "loss": 4.0406,
      "step": 83
    },
    {
      "epoch": 0.062360801781737196,
      "grad_norm": 4.266716480255127,
      "learning_rate": 3.851851851851852e-05,
      "loss": 4.0884,
      "step": 84
    },
    {
      "epoch": 0.06310319227913883,
      "grad_norm": 4.633064270019531,
      "learning_rate": 3.777777777777778e-05,
      "loss": 4.0394,
      "step": 85
    },
    {
      "epoch": 0.06384558277654045,
      "grad_norm": 3.8371613025665283,
      "learning_rate": 3.7037037037037037e-05,
      "loss": 4.0844,
      "step": 86
    },
    {
      "epoch": 0.0645879732739421,
      "grad_norm": 4.40987491607666,
      "learning_rate": 3.62962962962963e-05,
      "loss": 4.1044,
      "step": 87
    },
    {
      "epoch": 0.06533036377134373,
      "grad_norm": 4.026908874511719,
      "learning_rate": 3.555555555555556e-05,
      "loss": 3.9578,
      "step": 88
    },
    {
      "epoch": 0.06607275426874536,
      "grad_norm": 3.6779613494873047,
      "learning_rate": 3.481481481481482e-05,
      "loss": 3.9984,
      "step": 89
    },
    {
      "epoch": 0.066815144766147,
      "grad_norm": 3.7098305225372314,
      "learning_rate": 3.4074074074074077e-05,
      "loss": 4.0338,
      "step": 90
    },
    {
      "epoch": 0.06755753526354863,
      "grad_norm": 4.0180583000183105,
      "learning_rate": 3.3333333333333335e-05,
      "loss": 4.0072,
      "step": 91
    },
    {
      "epoch": 0.06829992576095026,
      "grad_norm": 3.5639853477478027,
      "learning_rate": 3.25925925925926e-05,
      "loss": 4.0788,
      "step": 92
    },
    {
      "epoch": 0.06904231625835189,
      "grad_norm": 4.294278621673584,
      "learning_rate": 3.185185185185185e-05,
      "loss": 4.0802,
      "step": 93
    },
    {
      "epoch": 0.06978470675575353,
      "grad_norm": 3.8786306381225586,
      "learning_rate": 3.111111111111111e-05,
      "loss": 4.0129,
      "step": 94
    },
    {
      "epoch": 0.07052709725315516,
      "grad_norm": 4.1382975578308105,
      "learning_rate": 3.037037037037037e-05,
      "loss": 3.9641,
      "step": 95
    },
    {
      "epoch": 0.07126948775055679,
      "grad_norm": 3.9253156185150146,
      "learning_rate": 2.962962962962963e-05,
      "loss": 3.9691,
      "step": 96
    },
    {
      "epoch": 0.07201187824795843,
      "grad_norm": 3.5676801204681396,
      "learning_rate": 2.8888888888888888e-05,
      "loss": 3.9255,
      "step": 97
    },
    {
      "epoch": 0.07275426874536006,
      "grad_norm": 3.7259485721588135,
      "learning_rate": 2.814814814814815e-05,
      "loss": 4.0707,
      "step": 98
    },
    {
      "epoch": 0.07349665924276169,
      "grad_norm": 4.437907695770264,
      "learning_rate": 2.7407407407407408e-05,
      "loss": 3.9647,
      "step": 99
    },
    {
      "epoch": 0.07423904974016332,
      "grad_norm": 4.298852920532227,
      "learning_rate": 2.6666666666666667e-05,
      "loss": 4.0238,
      "step": 100
    },
    {
      "epoch": 0.07498144023756496,
      "grad_norm": 5.448251247406006,
      "learning_rate": 2.5925925925925925e-05,
      "loss": 3.9091,
      "step": 101
    },
    {
      "epoch": 0.0757238307349666,
      "grad_norm": 5.073084354400635,
      "learning_rate": 2.5185185185185183e-05,
      "loss": 4.0696,
      "step": 102
    },
    {
      "epoch": 0.07646622123236822,
      "grad_norm": 5.2814860343933105,
      "learning_rate": 2.4444444444444445e-05,
      "loss": 3.9374,
      "step": 103
    },
    {
      "epoch": 0.07720861172976985,
      "grad_norm": 5.054629802703857,
      "learning_rate": 2.3703703703703707e-05,
      "loss": 3.9998,
      "step": 104
    },
    {
      "epoch": 0.0779510022271715,
      "grad_norm": 5.667908191680908,
      "learning_rate": 2.2962962962962965e-05,
      "loss": 4.1534,
      "step": 105
    },
    {
      "epoch": 0.07869339272457312,
      "grad_norm": 6.357358932495117,
      "learning_rate": 2.2222222222222223e-05,
      "loss": 3.9688,
      "step": 106
    },
    {
      "epoch": 0.07943578322197475,
      "grad_norm": 5.1835713386535645,
      "learning_rate": 2.148148148148148e-05,
      "loss": 4.0387,
      "step": 107
    },
    {
      "epoch": 0.0801781737193764,
      "grad_norm": 5.503433704376221,
      "learning_rate": 2.074074074074074e-05,
      "loss": 3.8983,
      "step": 108
    },
    {
      "epoch": 0.08092056421677803,
      "grad_norm": 4.520665645599365,
      "learning_rate": 2e-05,
      "loss": 3.9187,
      "step": 109
    },
    {
      "epoch": 0.08166295471417966,
      "grad_norm": 4.219847202301025,
      "learning_rate": 1.925925925925926e-05,
      "loss": 4.0838,
      "step": 110
    },
    {
      "epoch": 0.08240534521158129,
      "grad_norm": 4.457126617431641,
      "learning_rate": 1.8518518518518518e-05,
      "loss": 3.9964,
      "step": 111
    },
    {
      "epoch": 0.08314773570898293,
      "grad_norm": 5.287356853485107,
      "learning_rate": 1.777777777777778e-05,
      "loss": 3.8521,
      "step": 112
    },
    {
      "epoch": 0.08389012620638456,
      "grad_norm": 6.791639804840088,
      "learning_rate": 1.7037037037037038e-05,
      "loss": 3.9422,
      "step": 113
    },
    {
      "epoch": 0.08463251670378619,
      "grad_norm": 4.947393894195557,
      "learning_rate": 1.62962962962963e-05,
      "loss": 3.9414,
      "step": 114
    },
    {
      "epoch": 0.08537490720118783,
      "grad_norm": 4.619991779327393,
      "learning_rate": 1.5555555555555555e-05,
      "loss": 4.0153,
      "step": 115
    },
    {
      "epoch": 0.08611729769858946,
      "grad_norm": 5.148176193237305,
      "learning_rate": 1.4814814814814815e-05,
      "loss": 3.948,
      "step": 116
    },
    {
      "epoch": 0.08685968819599109,
      "grad_norm": 5.733783721923828,
      "learning_rate": 1.4074074074074075e-05,
      "loss": 4.1281,
      "step": 117
    },
    {
      "epoch": 0.08760207869339272,
      "grad_norm": 5.111457347869873,
      "learning_rate": 1.3333333333333333e-05,
      "loss": 4.0939,
      "step": 118
    },
    {
      "epoch": 0.08834446919079436,
      "grad_norm": 5.221336364746094,
      "learning_rate": 1.2592592592592592e-05,
      "loss": 3.8455,
      "step": 119
    },
    {
      "epoch": 0.08908685968819599,
      "grad_norm": 5.991607189178467,
      "learning_rate": 1.1851851851851853e-05,
      "loss": 3.9784,
      "step": 120
    },
    {
      "epoch": 0.08982925018559762,
      "grad_norm": 5.554050445556641,
      "learning_rate": 1.1111111111111112e-05,
      "loss": 4.0089,
      "step": 121
    },
    {
      "epoch": 0.09057164068299926,
      "grad_norm": 5.784102439880371,
      "learning_rate": 1.037037037037037e-05,
      "loss": 3.9384,
      "step": 122
    },
    {
      "epoch": 0.09131403118040089,
      "grad_norm": 4.838724613189697,
      "learning_rate": 9.62962962962963e-06,
      "loss": 4.0008,
      "step": 123
    },
    {
      "epoch": 0.09205642167780252,
      "grad_norm": 5.382025241851807,
      "learning_rate": 8.88888888888889e-06,
      "loss": 3.9238,
      "step": 124
    },
    {
      "epoch": 0.09279881217520415,
      "grad_norm": 5.029393672943115,
      "learning_rate": 8.14814814814815e-06,
      "loss": 3.9048,
      "step": 125
    },
    {
      "epoch": 0.0935412026726058,
      "grad_norm": 6.26558780670166,
      "learning_rate": 7.4074074074074075e-06,
      "loss": 3.9202,
      "step": 126
    },
    {
      "epoch": 0.09428359317000742,
      "grad_norm": 6.26575231552124,
      "learning_rate": 6.666666666666667e-06,
      "loss": 3.8389,
      "step": 127
    },
    {
      "epoch": 0.09502598366740905,
      "grad_norm": 5.972460746765137,
      "learning_rate": 5.925925925925927e-06,
      "loss": 4.0086,
      "step": 128
    },
    {
      "epoch": 0.0957683741648107,
      "grad_norm": 6.387014865875244,
      "learning_rate": 5.185185185185185e-06,
      "loss": 3.969,
      "step": 129
    },
    {
      "epoch": 0.09651076466221233,
      "grad_norm": 5.375814914703369,
      "learning_rate": 4.444444444444445e-06,
      "loss": 3.9586,
      "step": 130
    },
    {
      "epoch": 0.09725315515961395,
      "grad_norm": 4.643064022064209,
      "learning_rate": 3.7037037037037037e-06,
      "loss": 3.9271,
      "step": 131
    },
    {
      "epoch": 0.09799554565701558,
      "grad_norm": 5.163814067840576,
      "learning_rate": 2.9629629629629633e-06,
      "loss": 4.0623,
      "step": 132
    },
    {
      "epoch": 0.09873793615441723,
      "grad_norm": 5.551708698272705,
      "learning_rate": 2.2222222222222225e-06,
      "loss": 3.9546,
      "step": 133
    },
    {
      "epoch": 0.09948032665181886,
      "grad_norm": 4.923131942749023,
      "learning_rate": 1.4814814814814817e-06,
      "loss": 3.8291,
      "step": 134
    },
    {
      "epoch": 0.10022271714922049,
      "grad_norm": 5.77328634262085,
      "learning_rate": 7.407407407407408e-07,
      "loss": 3.9736,
      "step": 135
    }
  ],
  "logging_steps": 1,
  "max_steps": 135,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 1,
  "save_steps": 50,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 0.0,
  "train_batch_size": 64,
  "trial_name": null,
  "trial_params": null
}
