{
  "best_global_step": null,
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 0.044159858688452194,
  "eval_steps": 500,
  "global_step": 250,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.0001766394347538088,
      "grad_norm": 19.12651824951172,
      "learning_rate": 0.0005,
      "loss": 7.2457,
      "step": 1
    },
    {
      "epoch": 0.0003532788695076176,
      "grad_norm": 13.492229461669922,
      "learning_rate": 0.0004982394366197183,
      "loss": 6.8327,
      "step": 2
    },
    {
      "epoch": 0.0005299183042614264,
      "grad_norm": 15.111162185668945,
      "learning_rate": 0.0004964788732394367,
      "loss": 4.0629,
      "step": 3
    },
    {
      "epoch": 0.0007065577390152352,
      "grad_norm": 10.33081340789795,
      "learning_rate": 0.0004947183098591549,
      "loss": 3.2706,
      "step": 4
    },
    {
      "epoch": 0.0008831971737690439,
      "grad_norm": 5.5864691734313965,
      "learning_rate": 0.0004929577464788732,
      "loss": 2.405,
      "step": 5
    },
    {
      "epoch": 0.0010598366085228527,
      "grad_norm": 4.354925155639648,
      "learning_rate": 0.0004911971830985916,
      "loss": 2.2681,
      "step": 6
    },
    {
      "epoch": 0.0012364760432766616,
      "grad_norm": 5.14440393447876,
      "learning_rate": 0.0004894366197183099,
      "loss": 2.0419,
      "step": 7
    },
    {
      "epoch": 0.0014131154780304704,
      "grad_norm": 4.745134353637695,
      "learning_rate": 0.0004876760563380282,
      "loss": 2.1334,
      "step": 8
    },
    {
      "epoch": 0.001589754912784279,
      "grad_norm": 4.988801956176758,
      "learning_rate": 0.0004859154929577465,
      "loss": 2.1207,
      "step": 9
    },
    {
      "epoch": 0.0017663943475380878,
      "grad_norm": 5.320942401885986,
      "learning_rate": 0.0004841549295774648,
      "loss": 2.2925,
      "step": 10
    },
    {
      "epoch": 0.0019430337822918967,
      "grad_norm": 10.632095336914062,
      "learning_rate": 0.0004823943661971831,
      "loss": 1.7762,
      "step": 11
    },
    {
      "epoch": 0.0021196732170457055,
      "grad_norm": 7.535763263702393,
      "learning_rate": 0.00048063380281690145,
      "loss": 1.2855,
      "step": 12
    },
    {
      "epoch": 0.002296312651799514,
      "grad_norm": 7.355136394500732,
      "learning_rate": 0.0004788732394366197,
      "loss": 1.4243,
      "step": 13
    },
    {
      "epoch": 0.002472952086553323,
      "grad_norm": 9.005838394165039,
      "learning_rate": 0.000477112676056338,
      "loss": 1.3554,
      "step": 14
    },
    {
      "epoch": 0.0026495915213071318,
      "grad_norm": 8.25002670288086,
      "learning_rate": 0.0004753521126760563,
      "loss": 1.3676,
      "step": 15
    },
    {
      "epoch": 0.002826230956060941,
      "grad_norm": 5.066961765289307,
      "learning_rate": 0.00047359154929577465,
      "loss": 1.2083,
      "step": 16
    },
    {
      "epoch": 0.0030028703908147494,
      "grad_norm": 5.969293117523193,
      "learning_rate": 0.0004718309859154929,
      "loss": 1.448,
      "step": 17
    },
    {
      "epoch": 0.003179509825568558,
      "grad_norm": 4.635509014129639,
      "learning_rate": 0.00047007042253521125,
      "loss": 1.3505,
      "step": 18
    },
    {
      "epoch": 0.003356149260322367,
      "grad_norm": 4.794165134429932,
      "learning_rate": 0.0004683098591549296,
      "loss": 1.064,
      "step": 19
    },
    {
      "epoch": 0.0035327886950761757,
      "grad_norm": 4.129970550537109,
      "learning_rate": 0.0004665492957746479,
      "loss": 1.585,
      "step": 20
    },
    {
      "epoch": 0.0037094281298299847,
      "grad_norm": 5.141682147979736,
      "learning_rate": 0.0004647887323943662,
      "loss": 1.5168,
      "step": 21
    },
    {
      "epoch": 0.0038860675645837933,
      "grad_norm": 5.32069730758667,
      "learning_rate": 0.0004630281690140845,
      "loss": 1.1332,
      "step": 22
    },
    {
      "epoch": 0.004062706999337602,
      "grad_norm": 5.639753341674805,
      "learning_rate": 0.00046126760563380283,
      "loss": 1.0949,
      "step": 23
    },
    {
      "epoch": 0.004239346434091411,
      "grad_norm": 3.66644024848938,
      "learning_rate": 0.00045950704225352116,
      "loss": 0.6733,
      "step": 24
    },
    {
      "epoch": 0.00441598586884522,
      "grad_norm": 3.0663001537323,
      "learning_rate": 0.00045774647887323943,
      "loss": 1.0976,
      "step": 25
    },
    {
      "epoch": 0.004592625303599028,
      "grad_norm": 4.201491355895996,
      "learning_rate": 0.00045598591549295776,
      "loss": 0.9306,
      "step": 26
    },
    {
      "epoch": 0.004769264738352838,
      "grad_norm": 3.4257380962371826,
      "learning_rate": 0.0004542253521126761,
      "loss": 0.7342,
      "step": 27
    },
    {
      "epoch": 0.004945904173106646,
      "grad_norm": 3.3005592823028564,
      "learning_rate": 0.00045246478873239436,
      "loss": 0.8458,
      "step": 28
    },
    {
      "epoch": 0.005122543607860455,
      "grad_norm": 5.501459121704102,
      "learning_rate": 0.0004507042253521127,
      "loss": 0.9713,
      "step": 29
    },
    {
      "epoch": 0.0052991830426142635,
      "grad_norm": 7.7832536697387695,
      "learning_rate": 0.000448943661971831,
      "loss": 1.0846,
      "step": 30
    },
    {
      "epoch": 0.005475822477368072,
      "grad_norm": 5.959723949432373,
      "learning_rate": 0.00044718309859154934,
      "loss": 1.2444,
      "step": 31
    },
    {
      "epoch": 0.005652461912121882,
      "grad_norm": 4.577512741088867,
      "learning_rate": 0.0004454225352112676,
      "loss": 1.0794,
      "step": 32
    },
    {
      "epoch": 0.00582910134687569,
      "grad_norm": 4.502657413482666,
      "learning_rate": 0.00044366197183098594,
      "loss": 0.9046,
      "step": 33
    },
    {
      "epoch": 0.006005740781629499,
      "grad_norm": 4.823431015014648,
      "learning_rate": 0.00044190140845070427,
      "loss": 0.8546,
      "step": 34
    },
    {
      "epoch": 0.0061823802163833074,
      "grad_norm": 3.4585185050964355,
      "learning_rate": 0.00044014084507042255,
      "loss": 0.7373,
      "step": 35
    },
    {
      "epoch": 0.006359019651137116,
      "grad_norm": 2.4824721813201904,
      "learning_rate": 0.00043838028169014087,
      "loss": 0.7138,
      "step": 36
    },
    {
      "epoch": 0.0065356590858909255,
      "grad_norm": 4.425453186035156,
      "learning_rate": 0.00043661971830985915,
      "loss": 0.8536,
      "step": 37
    },
    {
      "epoch": 0.006712298520644734,
      "grad_norm": 3.796778917312622,
      "learning_rate": 0.0004348591549295775,
      "loss": 0.8093,
      "step": 38
    },
    {
      "epoch": 0.006888937955398543,
      "grad_norm": 3.3407235145568848,
      "learning_rate": 0.00043309859154929575,
      "loss": 0.703,
      "step": 39
    },
    {
      "epoch": 0.007065577390152351,
      "grad_norm": 4.053585052490234,
      "learning_rate": 0.0004313380281690141,
      "loss": 0.7068,
      "step": 40
    },
    {
      "epoch": 0.00724221682490616,
      "grad_norm": 2.3994579315185547,
      "learning_rate": 0.0004295774647887324,
      "loss": 0.5403,
      "step": 41
    },
    {
      "epoch": 0.0074188562596599695,
      "grad_norm": 4.273620128631592,
      "learning_rate": 0.0004278169014084507,
      "loss": 0.4893,
      "step": 42
    },
    {
      "epoch": 0.007595495694413778,
      "grad_norm": 3.0560123920440674,
      "learning_rate": 0.000426056338028169,
      "loss": 0.561,
      "step": 43
    },
    {
      "epoch": 0.007772135129167587,
      "grad_norm": 3.985389232635498,
      "learning_rate": 0.00042429577464788733,
      "loss": 0.9096,
      "step": 44
    },
    {
      "epoch": 0.007948774563921396,
      "grad_norm": 2.452608585357666,
      "learning_rate": 0.00042253521126760566,
      "loss": 0.4401,
      "step": 45
    },
    {
      "epoch": 0.008125413998675205,
      "grad_norm": 3.268113136291504,
      "learning_rate": 0.00042077464788732393,
      "loss": 0.6538,
      "step": 46
    },
    {
      "epoch": 0.008302053433429013,
      "grad_norm": 4.6615400314331055,
      "learning_rate": 0.00041901408450704226,
      "loss": 0.7818,
      "step": 47
    },
    {
      "epoch": 0.008478692868182822,
      "grad_norm": 3.0603771209716797,
      "learning_rate": 0.0004172535211267606,
      "loss": 0.6092,
      "step": 48
    },
    {
      "epoch": 0.00865533230293663,
      "grad_norm": 3.5405468940734863,
      "learning_rate": 0.00041549295774647886,
      "loss": 0.7233,
      "step": 49
    },
    {
      "epoch": 0.00883197173769044,
      "grad_norm": 3.3907389640808105,
      "learning_rate": 0.0004137323943661972,
      "loss": 0.5468,
      "step": 50
    },
    {
      "epoch": 0.009008611172444248,
      "grad_norm": 3.5232536792755127,
      "learning_rate": 0.0004119718309859155,
      "loss": 0.6832,
      "step": 51
    },
    {
      "epoch": 0.009185250607198056,
      "grad_norm": 3.7547924518585205,
      "learning_rate": 0.00041021126760563384,
      "loss": 0.4342,
      "step": 52
    },
    {
      "epoch": 0.009361890041951865,
      "grad_norm": 2.8304402828216553,
      "learning_rate": 0.0004084507042253521,
      "loss": 0.4359,
      "step": 53
    },
    {
      "epoch": 0.009538529476705675,
      "grad_norm": 3.0137219429016113,
      "learning_rate": 0.00040669014084507044,
      "loss": 0.4116,
      "step": 54
    },
    {
      "epoch": 0.009715168911459484,
      "grad_norm": 2.9732589721679688,
      "learning_rate": 0.00040492957746478877,
      "loss": 0.5251,
      "step": 55
    },
    {
      "epoch": 0.009891808346213293,
      "grad_norm": 2.711759090423584,
      "learning_rate": 0.0004031690140845071,
      "loss": 0.6492,
      "step": 56
    },
    {
      "epoch": 0.010068447780967101,
      "grad_norm": 2.8970754146575928,
      "learning_rate": 0.00040140845070422537,
      "loss": 0.5332,
      "step": 57
    },
    {
      "epoch": 0.01024508721572091,
      "grad_norm": 2.6366169452667236,
      "learning_rate": 0.0003996478873239437,
      "loss": 0.7099,
      "step": 58
    },
    {
      "epoch": 0.010421726650474718,
      "grad_norm": 2.5123143196105957,
      "learning_rate": 0.000397887323943662,
      "loss": 0.6461,
      "step": 59
    },
    {
      "epoch": 0.010598366085228527,
      "grad_norm": 2.40634822845459,
      "learning_rate": 0.0003961267605633803,
      "loss": 0.539,
      "step": 60
    },
    {
      "epoch": 0.010775005519982336,
      "grad_norm": 1.6987513303756714,
      "learning_rate": 0.00039436619718309857,
      "loss": 0.3193,
      "step": 61
    },
    {
      "epoch": 0.010951644954736144,
      "grad_norm": 2.417391061782837,
      "learning_rate": 0.0003926056338028169,
      "loss": 0.4751,
      "step": 62
    },
    {
      "epoch": 0.011128284389489953,
      "grad_norm": 2.0811100006103516,
      "learning_rate": 0.0003908450704225352,
      "loss": 0.4336,
      "step": 63
    },
    {
      "epoch": 0.011304923824243763,
      "grad_norm": 2.6264984607696533,
      "learning_rate": 0.0003890845070422535,
      "loss": 0.4485,
      "step": 64
    },
    {
      "epoch": 0.011481563258997572,
      "grad_norm": 2.5659780502319336,
      "learning_rate": 0.0003873239436619718,
      "loss": 0.5079,
      "step": 65
    },
    {
      "epoch": 0.01165820269375138,
      "grad_norm": 3.5890159606933594,
      "learning_rate": 0.00038556338028169015,
      "loss": 0.4714,
      "step": 66
    },
    {
      "epoch": 0.011834842128505189,
      "grad_norm": 3.067603349685669,
      "learning_rate": 0.0003838028169014084,
      "loss": 0.4086,
      "step": 67
    },
    {
      "epoch": 0.012011481563258998,
      "grad_norm": 4.112620830535889,
      "learning_rate": 0.00038204225352112675,
      "loss": 0.488,
      "step": 68
    },
    {
      "epoch": 0.012188120998012806,
      "grad_norm": 3.0702898502349854,
      "learning_rate": 0.0003802816901408451,
      "loss": 0.4415,
      "step": 69
    },
    {
      "epoch": 0.012364760432766615,
      "grad_norm": 2.54840350151062,
      "learning_rate": 0.0003785211267605634,
      "loss": 0.3515,
      "step": 70
    },
    {
      "epoch": 0.012541399867520424,
      "grad_norm": 3.1595160961151123,
      "learning_rate": 0.0003767605633802817,
      "loss": 0.386,
      "step": 71
    },
    {
      "epoch": 0.012718039302274232,
      "grad_norm": 3.43325138092041,
      "learning_rate": 0.000375,
      "loss": 0.4731,
      "step": 72
    },
    {
      "epoch": 0.01289467873702804,
      "grad_norm": 4.252902507781982,
      "learning_rate": 0.00037323943661971834,
      "loss": 0.5342,
      "step": 73
    },
    {
      "epoch": 0.013071318171781851,
      "grad_norm": 5.081543445587158,
      "learning_rate": 0.0003714788732394366,
      "loss": 0.8403,
      "step": 74
    },
    {
      "epoch": 0.01324795760653566,
      "grad_norm": 3.3130548000335693,
      "learning_rate": 0.00036971830985915494,
      "loss": 0.4232,
      "step": 75
    },
    {
      "epoch": 0.013424597041289468,
      "grad_norm": 4.1580681800842285,
      "learning_rate": 0.00036795774647887326,
      "loss": 0.5001,
      "step": 76
    },
    {
      "epoch": 0.013601236476043277,
      "grad_norm": 4.179632186889648,
      "learning_rate": 0.0003661971830985916,
      "loss": 0.4041,
      "step": 77
    },
    {
      "epoch": 0.013777875910797086,
      "grad_norm": 3.797220230102539,
      "learning_rate": 0.00036443661971830986,
      "loss": 0.4828,
      "step": 78
    },
    {
      "epoch": 0.013954515345550894,
      "grad_norm": 5.212714672088623,
      "learning_rate": 0.0003626760563380282,
      "loss": 0.4705,
      "step": 79
    },
    {
      "epoch": 0.014131154780304703,
      "grad_norm": 2.440980911254883,
      "learning_rate": 0.0003609154929577465,
      "loss": 0.3558,
      "step": 80
    },
    {
      "epoch": 0.014307794215058511,
      "grad_norm": 4.331256866455078,
      "learning_rate": 0.0003591549295774648,
      "loss": 0.5173,
      "step": 81
    },
    {
      "epoch": 0.01448443364981232,
      "grad_norm": 3.918773651123047,
      "learning_rate": 0.0003573943661971831,
      "loss": 0.4527,
      "step": 82
    },
    {
      "epoch": 0.014661073084566129,
      "grad_norm": 3.524308681488037,
      "learning_rate": 0.00035563380281690145,
      "loss": 0.4146,
      "step": 83
    },
    {
      "epoch": 0.014837712519319939,
      "grad_norm": 4.285606861114502,
      "learning_rate": 0.0003538732394366197,
      "loss": 0.4589,
      "step": 84
    },
    {
      "epoch": 0.015014351954073748,
      "grad_norm": 7.370967388153076,
      "learning_rate": 0.000352112676056338,
      "loss": 0.3901,
      "step": 85
    },
    {
      "epoch": 0.015190991388827556,
      "grad_norm": 2.888242721557617,
      "learning_rate": 0.0003503521126760563,
      "loss": 0.4432,
      "step": 86
    },
    {
      "epoch": 0.015367630823581365,
      "grad_norm": 3.3759219646453857,
      "learning_rate": 0.00034859154929577465,
      "loss": 0.4899,
      "step": 87
    },
    {
      "epoch": 0.015544270258335173,
      "grad_norm": 3.1833434104919434,
      "learning_rate": 0.0003468309859154929,
      "loss": 0.345,
      "step": 88
    },
    {
      "epoch": 0.015720909693088984,
      "grad_norm": 2.674530029296875,
      "learning_rate": 0.00034507042253521125,
      "loss": 0.4179,
      "step": 89
    },
    {
      "epoch": 0.015897549127842792,
      "grad_norm": 2.289351463317871,
      "learning_rate": 0.0003433098591549296,
      "loss": 0.5077,
      "step": 90
    },
    {
      "epoch": 0.0160741885625966,
      "grad_norm": 4.173052787780762,
      "learning_rate": 0.0003415492957746479,
      "loss": 0.4055,
      "step": 91
    },
    {
      "epoch": 0.01625082799735041,
      "grad_norm": 2.3406002521514893,
      "learning_rate": 0.0003397887323943662,
      "loss": 0.4283,
      "step": 92
    },
    {
      "epoch": 0.016427467432104218,
      "grad_norm": 3.603883981704712,
      "learning_rate": 0.0003380281690140845,
      "loss": 0.2657,
      "step": 93
    },
    {
      "epoch": 0.016604106866858027,
      "grad_norm": 2.499840259552002,
      "learning_rate": 0.00033626760563380283,
      "loss": 0.269,
      "step": 94
    },
    {
      "epoch": 0.016780746301611835,
      "grad_norm": 4.223026752471924,
      "learning_rate": 0.00033450704225352116,
      "loss": 0.4547,
      "step": 95
    },
    {
      "epoch": 0.016957385736365644,
      "grad_norm": 2.65291690826416,
      "learning_rate": 0.00033274647887323943,
      "loss": 0.47,
      "step": 96
    },
    {
      "epoch": 0.017134025171119453,
      "grad_norm": 3.2576777935028076,
      "learning_rate": 0.00033098591549295776,
      "loss": 0.4184,
      "step": 97
    },
    {
      "epoch": 0.01731066460587326,
      "grad_norm": 2.515702247619629,
      "learning_rate": 0.0003292253521126761,
      "loss": 0.2331,
      "step": 98
    },
    {
      "epoch": 0.01748730404062707,
      "grad_norm": 3.2642292976379395,
      "learning_rate": 0.00032746478873239436,
      "loss": 0.3628,
      "step": 99
    },
    {
      "epoch": 0.01766394347538088,
      "grad_norm": 2.681748867034912,
      "learning_rate": 0.0003257042253521127,
      "loss": 0.4107,
      "step": 100
    },
    {
      "epoch": 0.017840582910134687,
      "grad_norm": 3.852606773376465,
      "learning_rate": 0.000323943661971831,
      "loss": 0.4292,
      "step": 101
    },
    {
      "epoch": 0.018017222344888496,
      "grad_norm": 3.2436540126800537,
      "learning_rate": 0.00032218309859154934,
      "loss": 0.2924,
      "step": 102
    },
    {
      "epoch": 0.018193861779642304,
      "grad_norm": 2.963789224624634,
      "learning_rate": 0.0003204225352112676,
      "loss": 0.4727,
      "step": 103
    },
    {
      "epoch": 0.018370501214396113,
      "grad_norm": 3.792978286743164,
      "learning_rate": 0.00031866197183098594,
      "loss": 0.4132,
      "step": 104
    },
    {
      "epoch": 0.01854714064914992,
      "grad_norm": 3.168426990509033,
      "learning_rate": 0.00031690140845070427,
      "loss": 0.3795,
      "step": 105
    },
    {
      "epoch": 0.01872378008390373,
      "grad_norm": 4.106752395629883,
      "learning_rate": 0.00031514084507042254,
      "loss": 0.3624,
      "step": 106
    },
    {
      "epoch": 0.01890041951865754,
      "grad_norm": 2.3726227283477783,
      "learning_rate": 0.00031338028169014087,
      "loss": 0.31,
      "step": 107
    },
    {
      "epoch": 0.01907705895341135,
      "grad_norm": 2.1893486976623535,
      "learning_rate": 0.00031161971830985914,
      "loss": 0.4899,
      "step": 108
    },
    {
      "epoch": 0.01925369838816516,
      "grad_norm": 2.831357479095459,
      "learning_rate": 0.00030985915492957747,
      "loss": 0.3496,
      "step": 109
    },
    {
      "epoch": 0.019430337822918968,
      "grad_norm": 4.348010063171387,
      "learning_rate": 0.00030809859154929574,
      "loss": 0.4832,
      "step": 110
    },
    {
      "epoch": 0.019606977257672777,
      "grad_norm": 3.7904508113861084,
      "learning_rate": 0.00030633802816901407,
      "loss": 0.4551,
      "step": 111
    },
    {
      "epoch": 0.019783616692426585,
      "grad_norm": 3.381187677383423,
      "learning_rate": 0.0003045774647887324,
      "loss": 0.3555,
      "step": 112
    },
    {
      "epoch": 0.019960256127180394,
      "grad_norm": 3.05346941947937,
      "learning_rate": 0.00030281690140845067,
      "loss": 0.2995,
      "step": 113
    },
    {
      "epoch": 0.020136895561934202,
      "grad_norm": 3.518010139465332,
      "learning_rate": 0.000301056338028169,
      "loss": 0.3034,
      "step": 114
    },
    {
      "epoch": 0.02031353499668801,
      "grad_norm": 2.1820247173309326,
      "learning_rate": 0.0002992957746478873,
      "loss": 0.3139,
      "step": 115
    },
    {
      "epoch": 0.02049017443144182,
      "grad_norm": 3.523165702819824,
      "learning_rate": 0.00029753521126760565,
      "loss": 0.2908,
      "step": 116
    },
    {
      "epoch": 0.02066681386619563,
      "grad_norm": 2.911363363265991,
      "learning_rate": 0.0002957746478873239,
      "loss": 0.2462,
      "step": 117
    },
    {
      "epoch": 0.020843453300949437,
      "grad_norm": 4.784259796142578,
      "learning_rate": 0.00029401408450704225,
      "loss": 0.2831,
      "step": 118
    },
    {
      "epoch": 0.021020092735703245,
      "grad_norm": 3.1686365604400635,
      "learning_rate": 0.0002922535211267606,
      "loss": 0.3338,
      "step": 119
    },
    {
      "epoch": 0.021196732170457054,
      "grad_norm": 3.1542091369628906,
      "learning_rate": 0.00029049295774647885,
      "loss": 0.4418,
      "step": 120
    },
    {
      "epoch": 0.021373371605210863,
      "grad_norm": 2.325684070587158,
      "learning_rate": 0.0002887323943661972,
      "loss": 0.2508,
      "step": 121
    },
    {
      "epoch": 0.02155001103996467,
      "grad_norm": 3.9120051860809326,
      "learning_rate": 0.0002869718309859155,
      "loss": 0.4893,
      "step": 122
    },
    {
      "epoch": 0.02172665047471848,
      "grad_norm": 1.926259994506836,
      "learning_rate": 0.00028521126760563384,
      "loss": 0.217,
      "step": 123
    },
    {
      "epoch": 0.02190328990947229,
      "grad_norm": 4.202299118041992,
      "learning_rate": 0.0002834507042253521,
      "loss": 0.3926,
      "step": 124
    },
    {
      "epoch": 0.022079929344226097,
      "grad_norm": 6.179052829742432,
      "learning_rate": 0.00028169014084507044,
      "loss": 0.4056,
      "step": 125
    },
    {
      "epoch": 0.022256568778979906,
      "grad_norm": 3.375300407409668,
      "learning_rate": 0.00027992957746478876,
      "loss": 0.3879,
      "step": 126
    },
    {
      "epoch": 0.022433208213733714,
      "grad_norm": 2.7296314239501953,
      "learning_rate": 0.0002781690140845071,
      "loss": 0.3172,
      "step": 127
    },
    {
      "epoch": 0.022609847648487526,
      "grad_norm": 5.719778060913086,
      "learning_rate": 0.00027640845070422537,
      "loss": 0.4153,
      "step": 128
    },
    {
      "epoch": 0.022786487083241335,
      "grad_norm": 2.709355115890503,
      "learning_rate": 0.0002746478873239437,
      "loss": 0.1933,
      "step": 129
    },
    {
      "epoch": 0.022963126517995144,
      "grad_norm": 2.8384103775024414,
      "learning_rate": 0.000272887323943662,
      "loss": 0.3879,
      "step": 130
    },
    {
      "epoch": 0.023139765952748952,
      "grad_norm": 3.7084333896636963,
      "learning_rate": 0.0002711267605633803,
      "loss": 0.2945,
      "step": 131
    },
    {
      "epoch": 0.02331640538750276,
      "grad_norm": 2.8237574100494385,
      "learning_rate": 0.00026936619718309857,
      "loss": 0.335,
      "step": 132
    },
    {
      "epoch": 0.02349304482225657,
      "grad_norm": 3.9827051162719727,
      "learning_rate": 0.0002676056338028169,
      "loss": 0.3218,
      "step": 133
    },
    {
      "epoch": 0.023669684257010378,
      "grad_norm": 3.385094165802002,
      "learning_rate": 0.0002658450704225352,
      "loss": 0.2559,
      "step": 134
    },
    {
      "epoch": 0.023846323691764187,
      "grad_norm": 2.153416395187378,
      "learning_rate": 0.0002640845070422535,
      "loss": 0.326,
      "step": 135
    },
    {
      "epoch": 0.024022963126517995,
      "grad_norm": 2.6447460651397705,
      "learning_rate": 0.0002623239436619718,
      "loss": 0.3043,
      "step": 136
    },
    {
      "epoch": 0.024199602561271804,
      "grad_norm": 3.7616665363311768,
      "learning_rate": 0.00026056338028169015,
      "loss": 0.4367,
      "step": 137
    },
    {
      "epoch": 0.024376241996025613,
      "grad_norm": 2.671527147293091,
      "learning_rate": 0.0002588028169014084,
      "loss": 0.2962,
      "step": 138
    },
    {
      "epoch": 0.02455288143077942,
      "grad_norm": 1.8475030660629272,
      "learning_rate": 0.00025704225352112675,
      "loss": 0.2553,
      "step": 139
    },
    {
      "epoch": 0.02472952086553323,
      "grad_norm": 4.125621318817139,
      "learning_rate": 0.0002552816901408451,
      "loss": 0.3689,
      "step": 140
    },
    {
      "epoch": 0.02490616030028704,
      "grad_norm": 2.247509002685547,
      "learning_rate": 0.0002535211267605634,
      "loss": 0.2557,
      "step": 141
    },
    {
      "epoch": 0.025082799735040847,
      "grad_norm": 2.8482954502105713,
      "learning_rate": 0.0002517605633802817,
      "loss": 0.3163,
      "step": 142
    },
    {
      "epoch": 0.025259439169794656,
      "grad_norm": 2.893301248550415,
      "learning_rate": 0.00025,
      "loss": 0.4011,
      "step": 143
    },
    {
      "epoch": 0.025436078604548464,
      "grad_norm": 2.5188686847686768,
      "learning_rate": 0.00024823943661971833,
      "loss": 0.28,
      "step": 144
    },
    {
      "epoch": 0.025612718039302273,
      "grad_norm": 3.963454246520996,
      "learning_rate": 0.0002464788732394366,
      "loss": 0.3737,
      "step": 145
    },
    {
      "epoch": 0.02578935747405608,
      "grad_norm": 3.4915716648101807,
      "learning_rate": 0.00024471830985915493,
      "loss": 0.2859,
      "step": 146
    },
    {
      "epoch": 0.025965996908809894,
      "grad_norm": 2.8219776153564453,
      "learning_rate": 0.00024295774647887326,
      "loss": 0.256,
      "step": 147
    },
    {
      "epoch": 0.026142636343563702,
      "grad_norm": 2.9124274253845215,
      "learning_rate": 0.00024119718309859156,
      "loss": 0.2781,
      "step": 148
    },
    {
      "epoch": 0.02631927577831751,
      "grad_norm": 3.7546515464782715,
      "learning_rate": 0.00023943661971830986,
      "loss": 0.2483,
      "step": 149
    },
    {
      "epoch": 0.02649591521307132,
      "grad_norm": 3.1185758113861084,
      "learning_rate": 0.00023767605633802816,
      "loss": 0.392,
      "step": 150
    },
    {
      "epoch": 0.026672554647825128,
      "grad_norm": 2.858848810195923,
      "learning_rate": 0.00023591549295774646,
      "loss": 0.2961,
      "step": 151
    },
    {
      "epoch": 0.026849194082578937,
      "grad_norm": 4.024233341217041,
      "learning_rate": 0.0002341549295774648,
      "loss": 0.299,
      "step": 152
    },
    {
      "epoch": 0.027025833517332745,
      "grad_norm": 1.6574472188949585,
      "learning_rate": 0.0002323943661971831,
      "loss": 0.1874,
      "step": 153
    },
    {
      "epoch": 0.027202472952086554,
      "grad_norm": 3.178492307662964,
      "learning_rate": 0.00023063380281690142,
      "loss": 0.3177,
      "step": 154
    },
    {
      "epoch": 0.027379112386840362,
      "grad_norm": 2.5818300247192383,
      "learning_rate": 0.00022887323943661972,
      "loss": 0.2277,
      "step": 155
    },
    {
      "epoch": 0.02755575182159417,
      "grad_norm": 3.1508400440216064,
      "learning_rate": 0.00022711267605633804,
      "loss": 0.2881,
      "step": 156
    },
    {
      "epoch": 0.02773239125634798,
      "grad_norm": 2.0458436012268066,
      "learning_rate": 0.00022535211267605634,
      "loss": 0.2499,
      "step": 157
    },
    {
      "epoch": 0.027909030691101788,
      "grad_norm": 3.0933241844177246,
      "learning_rate": 0.00022359154929577467,
      "loss": 0.2251,
      "step": 158
    },
    {
      "epoch": 0.028085670125855597,
      "grad_norm": 3.8823211193084717,
      "learning_rate": 0.00022183098591549297,
      "loss": 0.3312,
      "step": 159
    },
    {
      "epoch": 0.028262309560609405,
      "grad_norm": 2.1810834407806396,
      "learning_rate": 0.00022007042253521127,
      "loss": 0.3349,
      "step": 160
    },
    {
      "epoch": 0.028438948995363214,
      "grad_norm": 4.87778377532959,
      "learning_rate": 0.00021830985915492957,
      "loss": 0.3136,
      "step": 161
    },
    {
      "epoch": 0.028615588430117023,
      "grad_norm": 2.1595661640167236,
      "learning_rate": 0.00021654929577464787,
      "loss": 0.162,
      "step": 162
    },
    {
      "epoch": 0.02879222786487083,
      "grad_norm": 2.5700888633728027,
      "learning_rate": 0.0002147887323943662,
      "loss": 0.3137,
      "step": 163
    },
    {
      "epoch": 0.02896886729962464,
      "grad_norm": 2.371403932571411,
      "learning_rate": 0.0002130281690140845,
      "loss": 0.2881,
      "step": 164
    },
    {
      "epoch": 0.02914550673437845,
      "grad_norm": 2.1874682903289795,
      "learning_rate": 0.00021126760563380283,
      "loss": 0.2065,
      "step": 165
    },
    {
      "epoch": 0.029322146169132257,
      "grad_norm": 3.7622718811035156,
      "learning_rate": 0.00020950704225352113,
      "loss": 0.3811,
      "step": 166
    },
    {
      "epoch": 0.02949878560388607,
      "grad_norm": 4.005804538726807,
      "learning_rate": 0.00020774647887323943,
      "loss": 0.3936,
      "step": 167
    },
    {
      "epoch": 0.029675425038639878,
      "grad_norm": 3.1747143268585205,
      "learning_rate": 0.00020598591549295776,
      "loss": 0.2909,
      "step": 168
    },
    {
      "epoch": 0.029852064473393686,
      "grad_norm": 3.25762677192688,
      "learning_rate": 0.00020422535211267606,
      "loss": 0.2578,
      "step": 169
    },
    {
      "epoch": 0.030028703908147495,
      "grad_norm": 1.3829163312911987,
      "learning_rate": 0.00020246478873239438,
      "loss": 0.1639,
      "step": 170
    },
    {
      "epoch": 0.030205343342901304,
      "grad_norm": 3.8064334392547607,
      "learning_rate": 0.00020070422535211268,
      "loss": 0.379,
      "step": 171
    },
    {
      "epoch": 0.030381982777655112,
      "grad_norm": 2.1094470024108887,
      "learning_rate": 0.000198943661971831,
      "loss": 0.3007,
      "step": 172
    },
    {
      "epoch": 0.03055862221240892,
      "grad_norm": 1.7184653282165527,
      "learning_rate": 0.00019718309859154928,
      "loss": 0.2264,
      "step": 173
    },
    {
      "epoch": 0.03073526164716273,
      "grad_norm": 3.8972420692443848,
      "learning_rate": 0.0001954225352112676,
      "loss": 0.3311,
      "step": 174
    },
    {
      "epoch": 0.030911901081916538,
      "grad_norm": 4.431710243225098,
      "learning_rate": 0.0001936619718309859,
      "loss": 0.3343,
      "step": 175
    },
    {
      "epoch": 0.031088540516670347,
      "grad_norm": 2.178478956222534,
      "learning_rate": 0.0001919014084507042,
      "loss": 0.1589,
      "step": 176
    },
    {
      "epoch": 0.03126517995142416,
      "grad_norm": 2.8200273513793945,
      "learning_rate": 0.00019014084507042254,
      "loss": 0.3273,
      "step": 177
    },
    {
      "epoch": 0.03144181938617797,
      "grad_norm": 1.9898438453674316,
      "learning_rate": 0.00018838028169014084,
      "loss": 0.1778,
      "step": 178
    },
    {
      "epoch": 0.031618458820931776,
      "grad_norm": 3.6295125484466553,
      "learning_rate": 0.00018661971830985917,
      "loss": 0.2492,
      "step": 179
    },
    {
      "epoch": 0.031795098255685585,
      "grad_norm": 4.058981895446777,
      "learning_rate": 0.00018485915492957747,
      "loss": 0.3407,
      "step": 180
    },
    {
      "epoch": 0.03197173769043939,
      "grad_norm": 2.9655392169952393,
      "learning_rate": 0.0001830985915492958,
      "loss": 0.2199,
      "step": 181
    },
    {
      "epoch": 0.0321483771251932,
      "grad_norm": 3.246134042739868,
      "learning_rate": 0.0001813380281690141,
      "loss": 0.2453,
      "step": 182
    },
    {
      "epoch": 0.03232501655994701,
      "grad_norm": 2.645380735397339,
      "learning_rate": 0.0001795774647887324,
      "loss": 0.2458,
      "step": 183
    },
    {
      "epoch": 0.03250165599470082,
      "grad_norm": 3.3989040851593018,
      "learning_rate": 0.00017781690140845072,
      "loss": 0.3265,
      "step": 184
    },
    {
      "epoch": 0.03267829542945463,
      "grad_norm": 1.6427757740020752,
      "learning_rate": 0.000176056338028169,
      "loss": 0.1901,
      "step": 185
    },
    {
      "epoch": 0.032854934864208436,
      "grad_norm": 2.126072883605957,
      "learning_rate": 0.00017429577464788732,
      "loss": 0.1595,
      "step": 186
    },
    {
      "epoch": 0.033031574298962245,
      "grad_norm": 2.8128671646118164,
      "learning_rate": 0.00017253521126760562,
      "loss": 0.2693,
      "step": 187
    },
    {
      "epoch": 0.033208213733716054,
      "grad_norm": 2.5412399768829346,
      "learning_rate": 0.00017077464788732395,
      "loss": 0.2382,
      "step": 188
    },
    {
      "epoch": 0.03338485316846986,
      "grad_norm": 3.234659194946289,
      "learning_rate": 0.00016901408450704225,
      "loss": 0.3004,
      "step": 189
    },
    {
      "epoch": 0.03356149260322367,
      "grad_norm": 3.2846570014953613,
      "learning_rate": 0.00016725352112676058,
      "loss": 0.2296,
      "step": 190
    },
    {
      "epoch": 0.03373813203797748,
      "grad_norm": 2.054755449295044,
      "learning_rate": 0.00016549295774647888,
      "loss": 0.1762,
      "step": 191
    },
    {
      "epoch": 0.03391477147273129,
      "grad_norm": 2.0686306953430176,
      "learning_rate": 0.00016373239436619718,
      "loss": 0.1747,
      "step": 192
    },
    {
      "epoch": 0.0340914109074851,
      "grad_norm": 3.181260108947754,
      "learning_rate": 0.0001619718309859155,
      "loss": 0.2815,
      "step": 193
    },
    {
      "epoch": 0.034268050342238905,
      "grad_norm": 1.838989019393921,
      "learning_rate": 0.0001602112676056338,
      "loss": 0.181,
      "step": 194
    },
    {
      "epoch": 0.034444689776992714,
      "grad_norm": 2.7535383701324463,
      "learning_rate": 0.00015845070422535213,
      "loss": 0.2844,
      "step": 195
    },
    {
      "epoch": 0.03462132921174652,
      "grad_norm": 1.841282606124878,
      "learning_rate": 0.00015669014084507043,
      "loss": 0.1594,
      "step": 196
    },
    {
      "epoch": 0.03479796864650033,
      "grad_norm": 3.394834280014038,
      "learning_rate": 0.00015492957746478874,
      "loss": 0.2645,
      "step": 197
    },
    {
      "epoch": 0.03497460808125414,
      "grad_norm": 4.73738956451416,
      "learning_rate": 0.00015316901408450704,
      "loss": 0.3431,
      "step": 198
    },
    {
      "epoch": 0.03515124751600795,
      "grad_norm": 3.2542245388031006,
      "learning_rate": 0.00015140845070422534,
      "loss": 0.2331,
      "step": 199
    },
    {
      "epoch": 0.03532788695076176,
      "grad_norm": 3.073452949523926,
      "learning_rate": 0.00014964788732394366,
      "loss": 0.2105,
      "step": 200
    },
    {
      "epoch": 0.035504526385515565,
      "grad_norm": 2.25585675239563,
      "learning_rate": 0.00014788732394366196,
      "loss": 0.2194,
      "step": 201
    },
    {
      "epoch": 0.035681165820269374,
      "grad_norm": 1.5271233320236206,
      "learning_rate": 0.0001461267605633803,
      "loss": 0.1896,
      "step": 202
    },
    {
      "epoch": 0.03585780525502318,
      "grad_norm": 2.071836233139038,
      "learning_rate": 0.0001443661971830986,
      "loss": 0.2067,
      "step": 203
    },
    {
      "epoch": 0.03603444468977699,
      "grad_norm": 5.733031272888184,
      "learning_rate": 0.00014260563380281692,
      "loss": 0.4085,
      "step": 204
    },
    {
      "epoch": 0.0362110841245308,
      "grad_norm": 3.14131236076355,
      "learning_rate": 0.00014084507042253522,
      "loss": 0.2509,
      "step": 205
    },
    {
      "epoch": 0.03638772355928461,
      "grad_norm": 2.698523998260498,
      "learning_rate": 0.00013908450704225355,
      "loss": 0.2659,
      "step": 206
    },
    {
      "epoch": 0.03656436299403842,
      "grad_norm": 5.610060214996338,
      "learning_rate": 0.00013732394366197185,
      "loss": 0.3461,
      "step": 207
    },
    {
      "epoch": 0.036741002428792226,
      "grad_norm": 2.312972068786621,
      "learning_rate": 0.00013556338028169015,
      "loss": 0.1736,
      "step": 208
    },
    {
      "epoch": 0.036917641863546034,
      "grad_norm": 3.52693772315979,
      "learning_rate": 0.00013380281690140845,
      "loss": 0.2526,
      "step": 209
    },
    {
      "epoch": 0.03709428129829984,
      "grad_norm": 2.7881205081939697,
      "learning_rate": 0.00013204225352112675,
      "loss": 0.1705,
      "step": 210
    },
    {
      "epoch": 0.03727092073305365,
      "grad_norm": 4.044778347015381,
      "learning_rate": 0.00013028169014084507,
      "loss": 0.3259,
      "step": 211
    },
    {
      "epoch": 0.03744756016780746,
      "grad_norm": 2.563873767852783,
      "learning_rate": 0.00012852112676056337,
      "loss": 0.1539,
      "step": 212
    },
    {
      "epoch": 0.03762419960256127,
      "grad_norm": 2.522697687149048,
      "learning_rate": 0.0001267605633802817,
      "loss": 0.196,
      "step": 213
    },
    {
      "epoch": 0.03780083903731508,
      "grad_norm": 1.8210413455963135,
      "learning_rate": 0.000125,
      "loss": 0.1657,
      "step": 214
    },
    {
      "epoch": 0.037977478472068886,
      "grad_norm": 2.808873414993286,
      "learning_rate": 0.0001232394366197183,
      "loss": 0.21,
      "step": 215
    },
    {
      "epoch": 0.0381541179068227,
      "grad_norm": 3.478498697280884,
      "learning_rate": 0.00012147887323943663,
      "loss": 0.2998,
      "step": 216
    },
    {
      "epoch": 0.03833075734157651,
      "grad_norm": 2.9189207553863525,
      "learning_rate": 0.00011971830985915493,
      "loss": 0.2686,
      "step": 217
    },
    {
      "epoch": 0.03850739677633032,
      "grad_norm": 5.1492600440979,
      "learning_rate": 0.00011795774647887323,
      "loss": 0.2974,
      "step": 218
    },
    {
      "epoch": 0.03868403621108413,
      "grad_norm": 2.492833375930786,
      "learning_rate": 0.00011619718309859154,
      "loss": 0.2088,
      "step": 219
    },
    {
      "epoch": 0.038860675645837936,
      "grad_norm": 3.2442660331726074,
      "learning_rate": 0.00011443661971830986,
      "loss": 0.2678,
      "step": 220
    },
    {
      "epoch": 0.039037315080591745,
      "grad_norm": 2.611790657043457,
      "learning_rate": 0.00011267605633802817,
      "loss": 0.1845,
      "step": 221
    },
    {
      "epoch": 0.03921395451534555,
      "grad_norm": 3.899268388748169,
      "learning_rate": 0.00011091549295774649,
      "loss": 0.3429,
      "step": 222
    },
    {
      "epoch": 0.03939059395009936,
      "grad_norm": 3.567655324935913,
      "learning_rate": 0.00010915492957746479,
      "loss": 0.212,
      "step": 223
    },
    {
      "epoch": 0.03956723338485317,
      "grad_norm": 2.546929359436035,
      "learning_rate": 0.0001073943661971831,
      "loss": 0.2465,
      "step": 224
    },
    {
      "epoch": 0.03974387281960698,
      "grad_norm": 3.8731977939605713,
      "learning_rate": 0.00010563380281690141,
      "loss": 0.2058,
      "step": 225
    },
    {
      "epoch": 0.03992051225436079,
      "grad_norm": 3.2971432209014893,
      "learning_rate": 0.00010387323943661971,
      "loss": 0.2777,
      "step": 226
    },
    {
      "epoch": 0.040097151689114596,
      "grad_norm": 3.3062362670898438,
      "learning_rate": 0.00010211267605633803,
      "loss": 0.1832,
      "step": 227
    },
    {
      "epoch": 0.040273791123868405,
      "grad_norm": 2.863740921020508,
      "learning_rate": 0.00010035211267605634,
      "loss": 0.2584,
      "step": 228
    },
    {
      "epoch": 0.040450430558622213,
      "grad_norm": 3.8695058822631836,
      "learning_rate": 9.859154929577464e-05,
      "loss": 0.2956,
      "step": 229
    },
    {
      "epoch": 0.04062706999337602,
      "grad_norm": 3.187278985977173,
      "learning_rate": 9.683098591549296e-05,
      "loss": 0.2171,
      "step": 230
    },
    {
      "epoch": 0.04080370942812983,
      "grad_norm": 3.6783711910247803,
      "learning_rate": 9.507042253521127e-05,
      "loss": 0.2462,
      "step": 231
    },
    {
      "epoch": 0.04098034886288364,
      "grad_norm": 3.3642210960388184,
      "learning_rate": 9.330985915492958e-05,
      "loss": 0.1909,
      "step": 232
    },
    {
      "epoch": 0.04115698829763745,
      "grad_norm": 1.791101098060608,
      "learning_rate": 9.15492957746479e-05,
      "loss": 0.1503,
      "step": 233
    },
    {
      "epoch": 0.04133362773239126,
      "grad_norm": 1.8587125539779663,
      "learning_rate": 8.97887323943662e-05,
      "loss": 0.1347,
      "step": 234
    },
    {
      "epoch": 0.041510267167145065,
      "grad_norm": 2.835951089859009,
      "learning_rate": 8.80281690140845e-05,
      "loss": 0.3244,
      "step": 235
    },
    {
      "epoch": 0.041686906601898874,
      "grad_norm": 1.4821302890777588,
      "learning_rate": 8.626760563380281e-05,
      "loss": 0.1819,
      "step": 236
    },
    {
      "epoch": 0.04186354603665268,
      "grad_norm": 3.5929739475250244,
      "learning_rate": 8.450704225352113e-05,
      "loss": 0.2086,
      "step": 237
    },
    {
      "epoch": 0.04204018547140649,
      "grad_norm": 3.2774555683135986,
      "learning_rate": 8.274647887323944e-05,
      "loss": 0.207,
      "step": 238
    },
    {
      "epoch": 0.0422168249061603,
      "grad_norm": 3.005859136581421,
      "learning_rate": 8.098591549295775e-05,
      "loss": 0.2434,
      "step": 239
    },
    {
      "epoch": 0.04239346434091411,
      "grad_norm": 2.3079264163970947,
      "learning_rate": 7.922535211267607e-05,
      "loss": 0.174,
      "step": 240
    },
    {
      "epoch": 0.04257010377566792,
      "grad_norm": 3.744246006011963,
      "learning_rate": 7.746478873239437e-05,
      "loss": 0.2331,
      "step": 241
    },
    {
      "epoch": 0.042746743210421725,
      "grad_norm": 2.164106845855713,
      "learning_rate": 7.570422535211267e-05,
      "loss": 0.1898,
      "step": 242
    },
    {
      "epoch": 0.042923382645175534,
      "grad_norm": 1.9362821578979492,
      "learning_rate": 7.394366197183098e-05,
      "loss": 0.099,
      "step": 243
    },
    {
      "epoch": 0.04310002207992934,
      "grad_norm": 1.6399319171905518,
      "learning_rate": 7.21830985915493e-05,
      "loss": 0.1754,
      "step": 244
    },
    {
      "epoch": 0.04327666151468315,
      "grad_norm": 2.637225866317749,
      "learning_rate": 7.042253521126761e-05,
      "loss": 0.2361,
      "step": 245
    },
    {
      "epoch": 0.04345330094943696,
      "grad_norm": 3.0238935947418213,
      "learning_rate": 6.866197183098592e-05,
      "loss": 0.1327,
      "step": 246
    },
    {
      "epoch": 0.04362994038419077,
      "grad_norm": 3.09609055519104,
      "learning_rate": 6.690140845070422e-05,
      "loss": 0.1997,
      "step": 247
    },
    {
      "epoch": 0.04380657981894458,
      "grad_norm": 2.627427816390991,
      "learning_rate": 6.514084507042254e-05,
      "loss": 0.1356,
      "step": 248
    },
    {
      "epoch": 0.043983219253698386,
      "grad_norm": 3.3278748989105225,
      "learning_rate": 6.338028169014085e-05,
      "loss": 0.2621,
      "step": 249
    },
    {
      "epoch": 0.044159858688452194,
      "grad_norm": 2.029092788696289,
      "learning_rate": 6.161971830985915e-05,
      "loss": 0.1608,
      "step": 250
    }
  ],
  "logging_steps": 1,
  "max_steps": 284,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 1,
  "save_steps": 50,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 1012115200545792.0,
  "train_batch_size": 8,
  "trial_name": null,
  "trial_params": null
}
